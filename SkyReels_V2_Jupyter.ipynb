{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# üé¨ SkyReels-V2 Video Generator\n",
				"\n",
				"This notebook provides an easy way to run SkyReels-V2 with a full-featured Gradio interface.\n",
				"\n",
				"## Features:\n",
				"- **Text-to-Video**: Generate videos from text descriptions\n",
				"- **Image-to-Video**: Animate static images into videos\n",
				"- **Diffusion Forcing**: Create long-form videos (up to 60+ seconds)\n",
				"- **Multiple Models**: 540P/720P, 1.3B/14B parameter variants\n",
				"- **Public Sharing**: Gradio creates public URLs for easy sharing\n",
				"\n",
				"## Quick Start:\n",
				"Just run the cells below in order!"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Step 1: Clone Repository (if not already done)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Clone the repository\n",
				"!git clone https://github.com/rdlogout/skyreel.git\n",
				"%cd skyreel"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Step 2: Automated Setup and Launch\n",
				"\n",
				"This will automatically:\n",
				"1. Detect your environment (Colab/Kaggle/Jupyter)\n",
				"2. Check for GPU availability\n",
				"3. Install all required dependencies\n",
				"4. Start the Gradio interface with public sharing"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Check GPU architecture first\n",
				"!nvidia-smi --query-gpu=name,compute_cap --format=csv,noheader\n",
				"\n",
				"# For RTX 6000 Blackwell (sm_120), we need special handling\n",
				"import subprocess\n",
				"import os\n",
				"\n",
				"# Set environment variables for Blackwell architecture\n",
				"os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
				"os.environ['TORCH_CUDA_ARCH_LIST'] = '5.0;6.0;7.0;7.5;8.0;8.6;9.0;12.0'\n",
				"\n",
				"print(\"üîß Environment configured for RTX 6000 Blackwell\")\n",
				"\n",
				"# Option 1: Use the complete installer (recommended for RTX 6000 Blackwell)\n",
				"!bash install_deps.sh\n",
				"\n",
				"# Option 2: Use the simple setup (alternative)\n",
				"# !bash simple_setup.sh full"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## RTX 6000 Blackwell Fix (If Needed)\n",
				"\n",
				"If you get CUDA sm_120 compatibility errors, run this cell:"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Fix for RTX 6000 Blackwell CUDA sm_120 compatibility\n",
				"import os\n",
				"import subprocess\n",
				"\n",
				"# Set environment variables\n",
				"os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
				"os.environ['TORCH_CUDA_ARCH_LIST'] = '5.0;6.0;7.0;7.5;8.0;8.6;9.0;12.0'\n",
				"\n",
				"print(\"üîß Installing PyTorch nightly for Blackwell architecture...\")\n",
				"\n",
				"# Install PyTorch nightly with sm_120 support\n",
				"!pip uninstall torch torchvision torchaudio -y\n",
				"!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu124\n",
				"\n",
				"# Verify installation\n",
				"import torch\n",
				"print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
				"print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
				"\n",
				"if torch.cuda.is_available():\n",
				"    for i in range(torch.cuda.device_count()):\n",
				"        props = torch.cuda.get_device_properties(i)\n",
				"        print(f\"‚úÖ GPU {i}: {torch.cuda.get_device_name(i)} (sm_{props.major}{props.minor})\")\n",
				"        \n",
				"    # Test basic CUDA operations\n",
				"    try:\n",
				"        x = torch.randn(10, 10).cuda()\n",
				"        y = torch.randn(10, 10).cuda()\n",
				"        z = torch.matmul(x, y)\n",
				"        print(\"‚úÖ CUDA operations working correctly\")\n",
				"    except Exception as e:\n",
				"        print(f\"‚ùå CUDA test failed: {e}\")\n",
				"else:\n",
				"    print(\"‚ùå CUDA not available\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Alternative: Manual Steps\n",
				"\n",
				"If you prefer to run steps manually:"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Step 1: Setup only (install dependencies)\n",
				"!bash simple_setup.sh deps"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Step 2: Test installation\n",
				"!bash simple_setup.sh test"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Step 3: Start Gradio with public sharing\n",
				"!bash simple_setup.sh run-share"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Direct Python Launch (Alternative)\n",
				"\n",
				"You can also start the interface directly from Python:"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Direct Python launch with public sharing\n",
				"import subprocess\n",
				"import sys\n",
				"\n",
				"# Start the Gradio app\n",
				"subprocess.run([sys.executable, \"gradio_app.py\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\", \"--share\"])"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Usage Instructions\n",
				"\n",
				"Once the Gradio interface is running, you'll see:\n",
				"\n",
				"1. **Local URL**: `http://localhost:7860` (for local access)\n",
				"2. **Public URL**: `https://xxxxx.gradio.live` (for sharing)\n",
				"\n",
				"### Generation Modes:\n",
				"\n",
				"#### Text-to-Video\n",
				"- Select \"Text-to-Video\" mode\n",
				"- Choose a model (e.g., SkyReels-V2-T2V-14B-540P)\n",
				"- Enter your prompt\n",
				"- Set duration and quality settings\n",
				"- Click \"Generate Video\"\n",
				"\n",
				"#### Image-to-Video\n",
				"- Select \"Image-to-Video\" mode\n",
				"- Upload an image\n",
				"- Add a descriptive prompt\n",
				"- Generate animated video\n",
				"\n",
				"#### Diffusion Forcing (Long Videos)\n",
				"- Select \"Diffusion Forcing\" mode\n",
				"- Enter a cinematic prompt\n",
				"- Set longer duration (10s, 30s, 60s)\n",
				"- Configure advanced settings for quality\n",
				"\n",
				"### Tips:\n",
				"- Use **1.3B models** for faster generation\n",
				"- Use **14B models** for better quality\n",
				"- Enable **TeaCache** for 2-3x speedup\n",
				"- Use **Prompt Enhancer** for better results\n",
				"- Start with shorter durations (4-10s) for testing"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## System Information"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Check system information\n",
				"import torch\n",
				"import psutil\n",
				"import os\n",
				"\n",
				"print(\"üñ•Ô∏è  System Information:\")\n",
				"print(f\"Python version: {sys.version}\")\n",
				"print(f\"PyTorch version: {torch.__version__}\")\n",
				"print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
				"\n",
				"if torch.cuda.is_available():\n",
				"    print(f\"CUDA version: {torch.version.cuda}\")\n",
				"    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
				"    for i in range(torch.cuda.device_count()):\n",
				"        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
				"        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
				"\n",
				"print(f\"\\nüíæ Memory: {psutil.virtual_memory().total / 1024**3:.1f} GB total\")\n",
				"print(f\"üíΩ Disk space: {psutil.disk_usage('/').free / 1024**3:.1f} GB free\")\n",
				"\n",
				"# Check if we're in a special environment\n",
				"if 'COLAB_GPU' in os.environ:\n",
				"    print(\"üî¨ Environment: Google Colab\")\n",
				"elif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
				"    print(\"üèÜ Environment: Kaggle\")\n",
				"else:\n",
				"    print(\"üìì Environment: Jupyter/Local\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Troubleshooting\n",
				"\n",
				"If you encounter issues:\n",
				"\n",
				"1. **Out of Memory**: Use smaller models (1.3B instead of 14B)\n",
				"2. **Slow Generation**: Enable TeaCache, reduce inference steps\n",
				"3. **Import Errors**: Run the setup script again\n",
				"4. **GPU Issues**: Check CUDA installation\n",
				"\n",
				"For more help, check the [GitHub repository](https://github.com/rdlogout/skyreel) or the [original SkyReels-V2 repo](https://github.com/SkyworkAI/SkyReels-V2)."
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.8.5"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
